{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8dacc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 3725 pairs to dialogue_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Path to raw text file\n",
    "input_file = \"dialogs.txt\"\n",
    "output_file = \"dialogue_dataset.csv\"\n",
    "\n",
    "# Open raw file and clean/process\n",
    "dialogue_pairs = []\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        # Skip empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        # Split by tab\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) != 2:\n",
    "            continue  # Ignore broken lines\n",
    "\n",
    "        prompt = parts[0].strip()\n",
    "        response = parts[1].strip()\n",
    "\n",
    "        # Skip too-short or empty messages\n",
    "        if len(prompt) < 2 or len(response) < 2:\n",
    "            continue\n",
    "\n",
    "        dialogue_pairs.append((prompt, response))\n",
    "\n",
    "# Write to CSV\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"prompt\", \"response\"])  # header\n",
    "    writer.writerows(dialogue_pairs)\n",
    "\n",
    "print(f\"âœ… Saved {len(dialogue_pairs)} pairs to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f8d281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding prompts with BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  BERT Chatbot Ready! Type 'exit' to quit.\n",
      "\n",
      "ðŸ” Matched: hi, how are you doing? (score=0.5620)\n",
      "Bot: i'm fine. how about yourself?\n",
      "\n",
      "ðŸ” Matched: who? (score=0.5372)\n",
      "Bot: two students from usc.\n",
      "\n",
      "ðŸ” Matched: i'm fine. how about yourself? (score=0.5573)\n",
      "Bot: i'm pretty good. thanks for asking.\n",
      "\n",
      "ðŸ” Matched: that's nice. (score=0.6213)\n",
      "Bot: are you going to be there?\n",
      "\n",
      "ðŸ” Matched: no. (score=0.8128)\n",
      "Bot: how do you know?\n",
      "\n",
      "ðŸ” Matched: no. (score=0.8128)\n",
      "Bot: how do you know?\n",
      "\n",
      "ðŸ” Matched: what's wrong? (score=0.6304)\n",
      "Bot: it's not orange!\n",
      "Goodbye ðŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"dialogue_dataset.csv\")\n",
    "\n",
    "# Drop nulls or weird entries\n",
    "df.dropna(inplace=True)\n",
    "df = df[df[\"prompt\"].str.strip() != \"\"]\n",
    "df = df[df[\"response\"].str.strip() != \"\"]\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Load pre-trained model (small but powerful)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode all prompts\n",
    "print(\"Encoding prompts with BERT...\")\n",
    "prompt_embeddings = model.encode(df['prompt'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Chat function\n",
    "def chat_with_bot(user_input, top_k=1):\n",
    "    # Encode user input\n",
    "    user_embedding = model.encode(user_input, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = util.cos_sim(user_embedding, prompt_embeddings)[0]\n",
    "\n",
    "    # Get top match\n",
    "    top_idx = torch.argmax(similarities).item()\n",
    "    matched_prompt = df.iloc[top_idx]['prompt']\n",
    "    response = df.iloc[top_idx]['response']\n",
    "    confidence = similarities[top_idx].item()\n",
    "\n",
    "    print(f\"\\nðŸ” Matched: {matched_prompt} (score={confidence:.4f})\")\n",
    "    return response\n",
    "\n",
    "# Test it\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nðŸ§  BERT Chatbot Ready! Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Goodbye ðŸ‘‹\")\n",
    "            break\n",
    "        bot_reply = chat_with_bot(user_input)\n",
    "        print(\"Bot:\", bot_reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b9f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model + tokenizer (DialoGPT small)\n",
    "model_name = \"microsoft/DialoGPT-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(\"ðŸ¤– Chatbot ready! Type 'quit' to exit.\\n\")\n",
    "\n",
    "# Chat loop\n",
    "chat_history_ids = None\n",
    "step = 0\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    # Encode input\n",
    "    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # Append to chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if step > 0 else new_input_ids\n",
    "\n",
    "    # Generate response\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        max_length=1000,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    bot_output = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    print(f\"Bot: {bot_output}\")\n",
    "\n",
    "    step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ea6df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
